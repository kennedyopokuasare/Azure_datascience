{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Pipeplines with Azure Machine Learning Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You use the Azure python SDK to orchestrate steps in a Pipeline will run in succession or in Parallel on a compute target.\n",
    "\n",
    "You'll need the latest version of the **azure.ai.ml** package to run the code in this notebook. Run the cell below to verify that it is installed.\n",
    "\n",
    "> **Note**:\n",
    "> If the **azure.ai.ml** package is not installed, run `pip install azure.ai.ml` to install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: azure-ai-ml\n",
      "Version: 1.20.0\n",
      "Summary: Microsoft Azure Machine Learning Client Library for Python\n",
      "Home-page: https://github.com/Azure/azure-sdk-for-python\n",
      "Author: Microsoft Corporation\n",
      "Author-email: azuresdkengsysadmins@microsoft.com\n",
      "License: MIT License\n",
      "Location: /opt/anaconda3/envs/automate/lib/python3.12/site-packages\n",
      "Requires: azure-common, azure-core, azure-mgmt-core, azure-storage-blob, azure-storage-file-datalake, azure-storage-file-share, colorama, isodate, jsonschema, marshmallow, msrest, opencensus-ext-azure, opencensus-ext-logging, pydash, pyjwt, pyyaml, strictyaml, tqdm, typing-extensions\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show azure.ai.ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to a Workspace\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription ID, resource group name, and workspace name. A `config.json` file containing these parameters can be downloaded from the Azure Machine Learning workspace or Azure portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: ../../config.json\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "ml_client = MLClient.from_config(\n",
    "    credential=DefaultAzureCredential(), path=\"../../config.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the dataset\n",
    "\n",
    "Azure Machine Learning providers several datastores that encapsulates a Dataset. Be considerate about the kind of datastores, use cases, and associated costs to determine the best datasource. Here we use the default datasource which is `blob` data store.\n",
    "\n",
    "**Authenticate with `Azure CLI` is required here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the dataset\n",
    "\n",
    "Azure Machine Learning providers several datastores that encapsulates a Dataset. Be considerate about the kind of datastores, use cases, and associated costs to determine the best datasource. Here we use the default datasource which is `blob` data store.\n",
    "\n",
    "**Authenticate with `Azure CLI` is required here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading diabetes.csv\u001b[32m (< 1 MB): 100%|██████████| 518k/518k [00:00<00:00, 987kB/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "file = \"../../data/diabetes.csv\"\n",
    "\n",
    "diabetes_data = Data(\n",
    "    name=\"diabetes_csv\",\n",
    "    path=file,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"Dataset for Diabetes model training\",\n",
    "    tags={\"source_type\": \"file\", \"source\": \"Local file\"},\n",
    "    version=\"1.0.0\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    diabetes_data = ml_client.data.create_or_update(diabetes_data)\n",
    "except Exception as ex:\n",
    "    print(\"Exception while registering dataset \", ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Component dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "components_dirs = {\n",
    "    \"eda\": \"./components/eda\",\n",
    "    \"data_prep\": \"./components/data_prep\",\n",
    "    \"train\": \"./components/train\",\n",
    "}\n",
    "\n",
    "for key, value in components_dirs.items():\n",
    "    os.makedirs(value, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/data_prep/data_prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {components_dirs[\"data_prep\"]}/data_prep.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import mlflow\n",
    "\n",
    "def main():\n",
    "    # Parse job parameters\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data', type=str)\n",
    "    parser.add_argument('--test_size', type=float, default=0.30)\n",
    "    parser.add_argument('--train_data', type=str)\n",
    "    parser.add_argument('--test_data', type=str)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Start logging\n",
    "    mlflow.start_run()\n",
    "\n",
    "    print(\" \".join(f\"{k}={v}\\n\" for k, v in vars(args).items()))\n",
    "\n",
    "    data = pd.read_csv(args.data, header=0)\n",
    "    print(\"num_samples:\", data.shape[0])\n",
    "    mlflow.log_metric(\"num_samples:\", data.shape[0])\n",
    "    print(\"num_features\", data.shape[1] - 1)\n",
    "    mlflow.log_metric(\"num_features\", data.shape[1] - 1)\n",
    "\n",
    "    train_data, test_data,  = train_test_split(data, test_size=args.test_size, random_state=0)\n",
    "\n",
    "    # args are mounted folders, so write the data into the folder\n",
    "    train_data.to_csv(os.path.join(args.train_data,\"train_data.csv\"), index=False)\n",
    "    test_data.to_csv(os.path.join(args.test_data,\"test_data.csv\"), index=False)\n",
    "\n",
    "    # Stop logging\n",
    "    mlflow.end_run()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis script\n",
    "\n",
    "Plot correlation, feature-wise distributions and pairwise scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/eda/diabetes-exploratory-plots.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {components_dirs[\"eda\"]}/diabetes-exploratory-plots.py\n",
    "# Plot distrubtions step\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "import argparse\n",
    "\n",
    "# Create a function that we can re-use\n",
    "def plot_correlations(data,output_path=\"outputs\"):\n",
    "    \"\"\"\n",
    "    This function will make a correlation graph and save it\n",
    "    \"\"\"\n",
    "    correlation = data.corr()\n",
    "    print(\"Correlation between features\\n\", correlation)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 12))\n",
    "    sns.heatmap(data=correlation, annot=True)\n",
    "    plt.title(\"Correlation betweeen features\")\n",
    "\n",
    "    # Save plot\n",
    "    filename = os.path.join(output_path, \"correlations-between-features.png\") \n",
    "    fig.savefig(filename)\n",
    "\n",
    "\n",
    "def plot_distribution(var_data, column_name=None, output_path=\"outputs\"):\n",
    "    \"\"\"\n",
    "    This function will make a distribution (graph) and save it\n",
    "    \"\"\"\n",
    "\n",
    "    # Get statistics\n",
    "    min_val = var_data.min()\n",
    "    max_val = var_data.max()\n",
    "    mean_val = var_data.mean()\n",
    "    med_val = var_data.median()\n",
    "    mod_val = var_data.mode()[0]\n",
    "\n",
    "    print(\n",
    "        \"{} Statistics:\\nMinimum:{:.2f}\\nMean:{:.2f}\\nMedian:{:.2f}\\nMode:{:.2f}\\nMaximum:{:.2f}\\n\".format(\n",
    "            \"\" if column_name is None else column_name,\n",
    "            min_val,\n",
    "            mean_val,\n",
    "            med_val,\n",
    "            mod_val,\n",
    "            max_val,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create a figure for 2 subplots (2 rows, 1 column)\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 4))\n",
    "\n",
    "    # Plot the histogram\n",
    "    ax[0].hist(var_data)\n",
    "    ax[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "    # Add lines for the mean, median, and mode\n",
    "    ax[0].axvline(x=min_val, color=\"gray\", linestyle=\"dashed\", linewidth=2, label=\"min\")\n",
    "    ax[0].axvline(x=mean_val, color=\"cyan\", linestyle=\"dashed\", linewidth=2, label = \"mean\")\n",
    "    ax[0].axvline(x=med_val, color=\"red\", linestyle=\"dashed\", linewidth=2, label = \"median\")\n",
    "    ax[0].axvline(x=mod_val, color=\"yellow\", linestyle=\"dashed\", linewidth=2, label = \"mode\")\n",
    "    ax[0].axvline(x=max_val, color=\"gray\", linestyle=\"dashed\", linewidth=2 , label = \"max\")\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Plot the boxplot\n",
    "    ax[1].boxplot(var_data, vert=False)\n",
    "    xlabel = \"Value\" if column_name is None else column_name\n",
    "    ax[1].set_xlabel(xlabel)\n",
    "\n",
    "    # Add a title to the Figure\n",
    "    title = (\n",
    "        \"Data Distribution\"\n",
    "        if column_name is None\n",
    "        else \"{} Data Distribution\".format(column_name)\n",
    "    )\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    # Save plot\n",
    "    filename = os.path.join(output_path,\"{}-distribution.png\".format(column_name))\n",
    "    fig.savefig(filename)\n",
    "\n",
    "\n",
    "def plot_scatters(x_y_data, output_path=\"outputs\"):\n",
    "    \"\"\"\n",
    "    Plot scatter plots with :y_column: on y-axis and save them. \n",
    "    \"\"\"\n",
    "    \n",
    "    x_column = x_y_data.columns.values[0]\n",
    "    y_column = x_y_data.columns.values[1]\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 12))\n",
    "    sns.regplot(data=x_y_data,x=x_column, y=y_column)\n",
    "    plt.xlabel(x_column)\n",
    "    plt.ylabel(y_column)\n",
    "    plt.title(\"Scatter plot of {} vs {}\".format(x_column,y_column))\n",
    "\n",
    "    # Save plot\n",
    "    filename = os.path.join(output_path,\"Scatter plot of {} vs {}.png\".format(x_column,y_column))\n",
    "    fig.savefig(filename)\n",
    "\n",
    "def main():\n",
    "    print(\"Loading Data...\")\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data', type=str)\n",
    "    parser.add_argument('--plots_dir', type=str)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    diabetes = pd.read_csv(args.data, header= 0)\n",
    "\n",
    "    # plot correlations\n",
    "    plot_correlations(data=diabetes, output_path = args.plots_dir)\n",
    "\n",
    "    # plot distributions\n",
    "    exlude_column = set([\"Diabetic\", \"PatientID\"])\n",
    "    columns = diabetes.columns.values\n",
    "    for x in columns:\n",
    "        if x not in exlude_column:\n",
    "            plot_distribution(var_data=diabetes[x],column_name=x, output_path = args.plots_dir)\n",
    "\n",
    "    # plot scatter plots\n",
    "    columns = set(columns)\n",
    "    column_comb=list(combinations(columns-exlude_column,2))\n",
    "    column_comb = [list(x) for x in column_comb]\n",
    "\n",
    "    for x_y_pairs in column_comb:\n",
    "        plot_scatters(diabetes[x_y_pairs], output_path = args.plots_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training script\n",
    "\n",
    "To train a model, you'll first create the **diabetes_training.py** script in the **src** folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/train/diabetes-training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {components_dirs[\"train\"]}/diabetes-training.py\n",
    "# import libraries\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import logging\n",
    "import mlflow\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Parse job parameters\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--reg_rate', type=float, default=0.01)\n",
    "    parser.add_argument('--train_data', type=str, help=\"path to train data\")\n",
    "    parser.add_argument('--test_data', type=str, help=\"path to test data\")\n",
    "    parser.add_argument(\"--trained_model\", type=str,help=\"path to trained data\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Start logging\n",
    "    mlflow.start_run()\n",
    "    \n",
    "    # load the diabetes dataset\n",
    "    print(\"Loading Data...\")\n",
    "    train_file = os.path.join(args.train_data, \"train_data.csv\")\n",
    "    test_file = os.path.join(args.test_data, \"test_data.csv\")\n",
    "    train_df = pd.read_csv(train_file, header=0, index_col=\"PatientID\")\n",
    "    y_train = train_df.pop(\"Diabetic\")\n",
    "    x_train = train_df.values\n",
    "    test_df = pd.read_csv(test_file, header=0, index_col=\"PatientID\")\n",
    "    y_test = test_df.pop(\"Diabetic\")\n",
    "    x_test = test_df.values\n",
    "\n",
    "    print(\"num_train_samples:\", x_train.shape[0])\n",
    "    mlflow.log_metric(\"num_train_samples:\", x_train.shape[0])\n",
    "    print(\"num_test_samples:\", x_test.shape[0])\n",
    "    mlflow.log_metric(\"num_test_samples\", x_test.shape[0])\n",
    "    print(\"num_features:\", x_train.shape[1])\n",
    "    mlflow.log_metric(\"num_features\", x_train.shape[1])\n",
    "    print(\"features:\", train_df.columns.values)\n",
    "    mlflow.log_param(\"features\", train_df.columns.values)\n",
    "\n",
    "\n",
    "    # train a logistic regression model\n",
    "    print('Training a logistic regression model with regularization rate of', args.reg_rate)\n",
    "    mlflow.log_param(\"estimator\",\"LogisticRegression\")\n",
    "    mlflow.log_param(\"regularization_rate\", args.reg_rate)\n",
    "    mlflow.log_param(\"solver\", \"liblinear\")\n",
    "    model = LogisticRegression(C=1/args.reg_rate, solver=\"liblinear\").fit(x_train, y_train)\n",
    "\n",
    "    # calculate accuracy\n",
    "    y_hat = model.predict(x_test)\n",
    "    acc = np.average(y_hat == y_test)\n",
    "    print('Accuracy:', float(acc))\n",
    "    mlflow.log_metric('Accuracy:', float(acc))\n",
    "\n",
    "    # calculate AUC\n",
    "    y_scores = model.predict_proba(x_test)\n",
    "    auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "    print('AUC:', str(auc))\n",
    "    mlflow.log_metric('AUC:', str(auc))\n",
    "\n",
    "    # Save the model to file\n",
    "    print(\"Saving model to file\")\n",
    "    filename = os.path.join(args.trained_model,\"model.pkl\")\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(model,file)\n",
    "\n",
    "    # Stop logging\n",
    "    mlflow.end_run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dependencies_dir = \"./dependencies\"\n",
    "os.makedirs(dependencies_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./dependencies/conda.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {dependencies_dir}/conda.yaml\n",
    "name: sklearn-1.5\n",
    "channels:\n",
    "- conda-forge\n",
    "- anaconda\n",
    "dependencies:\n",
    "- python=3.10\n",
    "- pip=21.3.1\n",
    "- pandas~=1.5.3\n",
    "- scipy~=1.10.0\n",
    "- numpy~=1.22.0\n",
    "- pip:\n",
    "  - scikit-learn-intelex==2024.6.0\n",
    "  - azureml-core==1.57.0\n",
    "  - azureml-defaults==1.57.0\n",
    "  - azureml-mlflow==1.57.0.post1\n",
    "  - azureml-telemetry==1.57.0\n",
    "  - scikit-learn~=1.5.0\n",
    "  - joblib~=1.2.0\n",
    "  # azureml-automl-common-tools packages\n",
    "  - py-spy==0.3.12\n",
    "  - debugpy~=1.6.3\n",
    "  - ipykernel~=6.0\n",
    "  - tensorboard\n",
    "  - psutil~=5.8.0\n",
    "  - matplotlib~=3.5.0\n",
    "  - seaborn~=0.13.2\n",
    "  - tqdm~=4.66.3\n",
    "  - py-cpuinfo==5.0.0\n",
    "  - torch-tb-profiler~=0.4.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment with name diabetes-scikit-learn is registered to workspace, the environment version is 4\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "env_name = \"diabetes-scikit-learn\"\n",
    "\n",
    "env = Environment(\n",
    "    name=env_name,\n",
    "    description=\"Custom environment for Diabetes prediction pipeline\",\n",
    "    tags={\"scikit-learn\": \"1.5.0\"},\n",
    "    conda_file=os.path.join(dependencies_dir, \"conda.yaml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    ")\n",
    "env = ml_client.environments.create_or_update(env)\n",
    "\n",
    "print(\n",
    "    f\"Environment with name {env.name} is registered to workspace, the environment version is {env.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component data_prep_diabetes with Version 2024-09-15-12-22-47-3858108 is registered\n",
      "Component eda_diabetes with Version 2024-09-15-12-22-51-8291496 is registered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading train (0.0 MBs): 100%|██████████| 2670/2670 [00:00<00:00, 16188.70it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component model_train_diabetes with Version 2024-09-15-12-22-57-9253183 is registered\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import command, Input, Output\n",
    "\n",
    "command_env = \"{0}:{1}\".format(env.name, env.version)\n",
    "experiment_name = \"diabetes-training\"\n",
    "\n",
    "data_prep_command = command(\n",
    "    name=\"data_prep_diabetes\",\n",
    "    display_name=\"Data preparation for training\",\n",
    "    description=\"Read the diabetes csv data and splits into train and test sets\",\n",
    "    inputs=dict(data=Input(type=\"uri_file\"), test_size=Input(type=\"number\")),\n",
    "    outputs=dict(\n",
    "        train_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        test_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    ),\n",
    "    code=components_dirs[\"data_prep\"],\n",
    "    command=\"python data_prep.py  --data ${{inputs.data}} --test_size ${{inputs.test_size}}  --train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}}\",\n",
    "    environment=command_env,\n",
    "    experiment_name=experiment_name,\n",
    ")\n",
    "\n",
    "# Now we register the component to the workspace\n",
    "data_prep_component = ml_client.create_or_update(data_prep_command.component)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {data_prep_component.name} with Version {data_prep_component.version} is registered\"\n",
    ")\n",
    "\n",
    "eda_command = command(\n",
    "    name=\"eda_diabetes\",\n",
    "    display_name=\"Exploratory data analysis\",\n",
    "    description=\"Reads the diabetes csv data and plot exploratory data analysis graphs\",\n",
    "    inputs=dict(data=Input(type=\"uri_file\")),\n",
    "    outputs=dict(plots_dir=Output(type=\"uri_folder\", mode=\"rw_mount\")),\n",
    "    code=components_dirs[\"eda\"],\n",
    "    command=\"python diabetes-exploratory-plots.py --data ${{inputs.data}} --plots ${{outputs.plots_dir}}\",\n",
    "    environment=command_env,\n",
    "    experiment_name=experiment_name,\n",
    ")\n",
    "\n",
    "# Now we register the component to the workspace\n",
    "eda_command_component = ml_client.create_or_update(eda_command.component)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {eda_command_component.name} with Version {eda_command_component.version} is registered\"\n",
    ")\n",
    "\n",
    "model_train_command = command(\n",
    "    name=\"model_train_diabetes\",\n",
    "    display_name=\"Train model\",\n",
    "    description=\"Train a Logistic Regression model diabetes dataset\",\n",
    "    inputs=dict(\n",
    "        train_data=Input(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        test_data=Input(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        reg_rate=Input(type=\"number\"),\n",
    "    ),\n",
    "    outputs=dict(trained_model=Output(type=\"uri_folder\", mode=\"rw_mount\")),\n",
    "    code=components_dirs[\"train\"],\n",
    "    command=\"python diabetes-training.py --train_data ${{inputs.train_data}} --test_data ${{inputs.test_data}} --reg_rate ${{inputs.reg_rate}} --trained_model ${{outputs.trained_model}}\",\n",
    "    environment=command_env,\n",
    "    experiment_name=experiment_name,\n",
    ")\n",
    "\n",
    "# Now we register the component to the workspace\n",
    "model_train_command_component = ml_client.create_or_update(\n",
    "    model_train_command.component\n",
    ")\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {model_train_command_component.name} with Version {model_train_command_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Pipeline\n",
    "Test data size (`test_size`) and Regularization rate (`reg_rate`) for the `LogisticRegression` are passed as parameters. Other parameters such as a registered dataset in a data store could also be passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import dsl, Input\n",
    "\n",
    "# Define the pipeline\n",
    "@dsl.pipeline(compute=\"serverless\", description=\" Diabetes prediction pipeline\")\n",
    "def diabetes_pipeline(data_input, test_size, reg_rate):\n",
    "    data_prep_job =data_prep_component(\n",
    "                data = data_input,\n",
    "                test_size = test_size\n",
    "    )\n",
    "\n",
    "    eda_job = eda_command_component(\n",
    "        data = data_input\n",
    "    )\n",
    "\n",
    "    model_train_job = model_train_command_component(\n",
    "        train_data = data_prep_job.outputs.train_data,\n",
    "        test_data = data_prep_job.outputs.test_data,\n",
    "        reg_rate = reg_rate\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"train_data\": data_prep_job.outputs.train_data,\n",
    "        \"test_data\": data_prep_job.outputs.test_data,\n",
    "        \"eda_plots\": eda_job.outputs.plots_dir,\n",
    "        \"trained_model\": model_train_job.outputs.trained_model\n",
    "    }\n",
    "\n",
    "\n",
    "# Define arguments / parameters\n",
    "diabetes_data = ml_client.data.get(\"diabetes_csv\", version=\"1.0.0\")\n",
    "test_size = 0.30\n",
    "reg_rate = 0.01\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = diabetes_pipeline(\n",
    "    data_input=Input(type=\"uri_file\", path= diabetes_data.path),\n",
    "    test_size=test_size,\n",
    "    reg_rate=reg_rate\n",
    ")\n",
    "\n",
    "#submit the pipeline job\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    experiment_name= experiment_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"03.completed_pipeline.png\" alt=\"drawing\" width=\"500\"/>\n",
    "<img src=\"06.Distribution_plots.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "# Open job url\n",
    "webbrowser.open(pipeline_job.studio_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection and Explainability with Automated Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncluster_basic = AmlCompute(\\n    name=compute_name,\\n    type=\"amlcompute\",\\n    size=\"Standard_A1_v2\",\\n    min_instances=0,\\n    max_instances=4,\\n    idle_time_before_scale_down=120,\\n)\\nml_client.begin_create_or_update(cluster_basic)\\n'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from azure.ai.ml.entities import ComputeInstance, AmlCompute\n",
    "\n",
    "\n",
    "compute_name = \"diabest-compute\"\n",
    "\n",
    "computeInstance = ComputeInstance(\n",
    "    name=compute_name, \n",
    "    size=\"Standard_A1_v2\", \n",
    "    idle_time_before_shutdown_minutes=30\n",
    ")\n",
    "\n",
    "ml_client.begin_create_or_update(computeInstance)\n",
    "\n",
    "\"\"\"\n",
    "cluster_basic = AmlCompute(\n",
    "    name=compute_name,\n",
    "    type=\"amlcompute\",\n",
    "    size=\"Standard_A1_v2\",\n",
    "    min_instances=0,\n",
    "    max_instances=4,\n",
    "    idle_time_before_scale_down=120,\n",
    ")\n",
    "ml_client.begin_create_or_update(cluster_basic)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated ML requires MLTable. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'azureml://datastores/workspaceblobstore/paths/LocalUpload/0e528e47e39c2caeecb4c8d2c9d3a61f/diabetes.mltable'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.entities import Data\n",
    "#Create MLTable from Diabetes data asset\n",
    "\n",
    "# Define the Data asset object\n",
    "diabetes_mlt = Data(\n",
    "    path=\"../../data/diabetes.mltable\",\n",
    "    type=AssetTypes.MLTABLE,\n",
    "    description=\"Diabetes table data in mltable format\",\n",
    "    name=\"diabetes_mlt2\",\n",
    "    version=\"2\",\n",
    ")\n",
    "ml_client.data.create_or_update(diabetes_mlt)\n",
    "diabetes_mlt.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import automl, Input\n",
    "\n",
    "diabetes_data_ml_table = Input(\n",
    "    type= AssetTypes.MLTABLE,\n",
    "    path= diabetes_mlt.path\n",
    ") \n",
    "\n",
    "# Configure the classification job\n",
    "classification_job = automl.classification(\n",
    "    compute=compute_name,\n",
    "    experiment_name=experiment_name,\n",
    "    training_data=diabetes_data_ml_table,\n",
    "    target_column_name=\"Diabetic\",\n",
    "    primary_metric=\"AUC_weighted\",\n",
    "    n_cross_validations=5,\n",
    "    enable_model_explainability=True,\n",
    "    tags={\"data\": \"diabetes_csv\"}\n",
    ")\n",
    "\n",
    "# set job limits. These are optional\n",
    "classification_job.set_limits(\n",
    "    timeout_minutes=600, \n",
    "    trial_timeout_minutes=20, \n",
    "    max_trials=5,\n",
    "    enable_early_termination=True,\n",
    ")\n",
    "\n",
    "# set training properties. Do not use logistic regression\n",
    "classification_job.set_training(\n",
    "    blocked_training_algorithms=[\"logistic_regression\"], \n",
    "    enable_onnx_compatible_models=True\n",
    ")\n",
    "\n",
    "# Submit the AutoML job\n",
    "returned_job = ml_client.jobs.create_or_update(\n",
    "    classification_job\n",
    ")  # submit the job to the backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ml.azure.com/runs/witty_car_tfcm4m1v7l?wsid=/subscriptions/ed463f81-92a5-476c-b6e1-82f1a28d21e2/resourcegroups/ml-prod-scale/workspaces/diabetes_prediction&tid=746a21b3-a76e-4d67-a142-eeb97ab5314f'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a URL for the status of the job\n",
    "returned_job.services[\"Studio\"].endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More reading more: \n",
    "- IntepretML : https://interpret.ml/\n",
    "- FairLearn: https://fairlearn.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
