{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Pipeplines with Azure Machine Learning Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You use the Azure python SDK to orchestrate steps in a Pipeline will run in succession or in Parallel on a compute target.\n",
    "\n",
    "You'll need the latest version of the **azure.ai.ml** package to run the code in this notebook. Run the cell below to verify that it is installed.\n",
    "\n",
    "> **Note**:\n",
    "> If the **azure.ai.ml** package is not installed, run `pip install azure.ai.ml` to install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip show azure.ai.ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to a Workspace\n",
    "\n",
    "To connect to a workspace, we need identifier parameters - a subscription ID, resource group name, and workspace name. A `config.json` file containing these parameters can be downloaded from the Azure Machine Learning workspace or Azure portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: ../../config.json\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "ml_client = MLClient.from_config(\n",
    "    credential=DefaultAzureCredential(), path=\"../../config.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the dataset\n",
    "\n",
    "Azure Machine Learning providers several datastores that encapsulates a Dataset. Be considerate about the kind of datastores, use cases, and associated costs to determine the best datasource. Here we use the default datasource which is `blob` data store.\n",
    "\n",
    "**Authenticate with `Azure CLI` is required here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the dataset\n",
    "\n",
    "Azure Machine Learning providers several datastores that encapsulates a Dataset. Be considerate about the kind of datastores, use cases, and associated costs to determine the best datasource. Here we use the default datasource which is `blob` data store.\n",
    "\n",
    "**Authenticate with `Azure CLI` is required here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception while registering dataset  (UserError) A data version with this name and version already exists. If you are trying to create a new data version, use a different name or version. If you are trying to update an existing data version, the existing asset's data uri cannot be changed. Only tags, description, and isArchived can be updated.\n",
      "Code: UserError\n",
      "Message: A data version with this name and version already exists. If you are trying to create a new data version, use a different name or version. If you are trying to update an existing data version, the existing asset's data uri cannot be changed. Only tags, description, and isArchived can be updated.\n",
      "Additional Information:Type: ComponentName\n",
      "Info: {\n",
      "    \"value\": \"managementfrontend\"\n",
      "}Type: Correlation\n",
      "Info: {\n",
      "    \"value\": {\n",
      "        \"operation\": \"f50768dc8433c56720f70ba5ac968802\",\n",
      "        \"request\": \"09a2023dc8713835\"\n",
      "    }\n",
      "}Type: Environment\n",
      "Info: {\n",
      "    \"value\": \"northeurope\"\n",
      "}Type: Location\n",
      "Info: {\n",
      "    \"value\": \"northeurope\"\n",
      "}Type: Time\n",
      "Info: {\n",
      "    \"value\": \"2024-09-15T07:49:25.9651249+00:00\"\n",
      "}Type: InnerError\n",
      "Info: {\n",
      "    \"value\": {\n",
      "        \"code\": \"Immutable\",\n",
      "        \"innerError\": {\n",
      "            \"code\": \"DataVersionPropertyImmutable\",\n",
      "            \"innerError\": null\n",
      "        }\n",
      "    }\n",
      "}Type: MessageFormat\n",
      "Info: {\n",
      "    \"value\": \"A data version with this name and version already exists. If you are trying to create a new data version, use a different name or version. If you are trying to update an existing data version, the existing asset's {property} cannot be changed. Only tags, description, and isArchived can be updated.\"\n",
      "}Type: MessageParameters\n",
      "Info: {\n",
      "    \"value\": {\n",
      "        \"property\": \"data uri\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "file = \"../../data/diabetes.csv\"\n",
    "\n",
    "diabetes_data = Data(\n",
    "    name=\"diabetes_csv\",\n",
    "    path=file,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"Dataset for Diabetes model training\",\n",
    "    tags={\"source_type\": \"file\", \"source\": \"Local file\"},\n",
    "    version=\"1.0.0\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    diabetes_data = ml_client.data.create_or_update(diabetes_data)\n",
    "except Exception as ex:\n",
    "    print(\"Exception while registering dataset \", ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Component dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "components_dirs = {\n",
    "    \"eda\": \"./components/eda\",\n",
    "    \"data_prep\": \"./components/data_prep\",\n",
    "    \"train\": \"./components/train\",\n",
    "}\n",
    "\n",
    "for key, value in components_dirs.items():\n",
    "    os.makedirs(value, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/data_prep/data_prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {components_dirs[\"data_prep\"]}/data_prep.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import mlflow\n",
    "\n",
    "def main():\n",
    "    # Parse job parameters\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data', type=str)\n",
    "    parser.add_argument('--test_size', type=float, default=0.30)\n",
    "    parser.add_argument('--train_data', type=str)\n",
    "    parser.add_argument('--test_data', type=str)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Start logging\n",
    "    mlflow.start_run()\n",
    "\n",
    "    print(\" \".join(f\"{k}={v}\\n\" for k, v in vars(args).items()))\n",
    "\n",
    "    data = pd.read_csv(args.data, header=0)\n",
    "    print(\"num_samples:\", data.shape[0])\n",
    "    mlflow.log_metric(\"num_samples:\", data.shape[0])\n",
    "    print(\"num_features\", data.shape[1] - 1)\n",
    "    mlflow.log_metric(\"num_features\", data.shape[1] - 1)\n",
    "\n",
    "    train_data, test_data,  = train_test_split(data, test_size=args.test_size, random_state=0)\n",
    "\n",
    "    # args are mounted folders, so write the data into the folder\n",
    "    train_data.to_csv(os.path.join(args.train_data,\"train_data.csv\"), index=False)\n",
    "    test_data.to_csv(os.path.join(args.test_data,\"test_data.csv\"), index=False)\n",
    "\n",
    "    # Stop logging\n",
    "    mlflow.end_run()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis script\n",
    "\n",
    "Plot correlation, feature-wise distributions and pairwise scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/eda/diabetes-exploratory-plots.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {components_dirs[\"eda\"]}/diabetes-exploratory-plots.py\n",
    "# Plot distrubtions step\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "import argparse\n",
    "\n",
    "# Create a function that we can re-use\n",
    "def plot_correlations(data,output_path=\"outputs\"):\n",
    "    \"\"\"\n",
    "    This function will make a correlation graph and save it\n",
    "    \"\"\"\n",
    "    correlation = data.corr()\n",
    "    print(\"Correlation between features\\n\", correlation)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 12))\n",
    "    sns.heatmap(data=correlation, annot=True)\n",
    "    plt.title(\"Correlation betweeen features\")\n",
    "\n",
    "    # Save plot\n",
    "    filename = os.path.join(output_path, \"correlations-between-features.png\") \n",
    "    fig.savefig(filename)\n",
    "\n",
    "\n",
    "def plot_distribution(var_data, column_name=None, output_path=\"outputs\"):\n",
    "    \"\"\"\n",
    "    This function will make a distribution (graph) and save it\n",
    "    \"\"\"\n",
    "\n",
    "    # Get statistics\n",
    "    min_val = var_data.min()\n",
    "    max_val = var_data.max()\n",
    "    mean_val = var_data.mean()\n",
    "    med_val = var_data.median()\n",
    "    mod_val = var_data.mode()[0]\n",
    "\n",
    "    print(\n",
    "        \"{} Statistics:\\nMinimum:{:.2f}\\nMean:{:.2f}\\nMedian:{:.2f}\\nMode:{:.2f}\\nMaximum:{:.2f}\\n\".format(\n",
    "            \"\" if column_name is None else column_name,\n",
    "            min_val,\n",
    "            mean_val,\n",
    "            med_val,\n",
    "            mod_val,\n",
    "            max_val,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create a figure for 2 subplots (2 rows, 1 column)\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 4))\n",
    "\n",
    "    # Plot the histogram\n",
    "    ax[0].hist(var_data)\n",
    "    ax[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "    # Add lines for the mean, median, and mode\n",
    "    ax[0].axvline(x=min_val, color=\"gray\", linestyle=\"dashed\", linewidth=2, label=\"min\")\n",
    "    ax[0].axvline(x=mean_val, color=\"cyan\", linestyle=\"dashed\", linewidth=2, label = \"mean\")\n",
    "    ax[0].axvline(x=med_val, color=\"red\", linestyle=\"dashed\", linewidth=2, label = \"median\")\n",
    "    ax[0].axvline(x=mod_val, color=\"yellow\", linestyle=\"dashed\", linewidth=2, label = \"mode\")\n",
    "    ax[0].axvline(x=max_val, color=\"gray\", linestyle=\"dashed\", linewidth=2 , label = \"max\")\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Plot the boxplot\n",
    "    ax[1].boxplot(var_data, vert=False)\n",
    "    xlabel = \"Value\" if column_name is None else column_name\n",
    "    ax[1].set_xlabel(xlabel)\n",
    "\n",
    "    # Add a title to the Figure\n",
    "    title = (\n",
    "        \"Data Distribution\"\n",
    "        if column_name is None\n",
    "        else \"{} Data Distribution\".format(column_name)\n",
    "    )\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    # Save plot\n",
    "    filename = os.path.join(output_path,\"{}-distribution.png\".format(column_name))\n",
    "    fig.savefig(filename)\n",
    "\n",
    "\n",
    "def plot_scatters(x_y_data, output_path=\"outputs\"):\n",
    "    \"\"\"\n",
    "    Plot scatter plots with :y_column: on y-axis and save them. \n",
    "    \"\"\"\n",
    "    \n",
    "    x_column = x_y_data.columns.values[0]\n",
    "    y_column = x_y_data.columns.values[1]\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 12))\n",
    "    sns.regplot(data=x_y_data,x=x_column, y=y_column)\n",
    "    plt.xlabel(x_column)\n",
    "    plt.ylabel(y_column)\n",
    "    plt.title(\"Scatter plot of {} vs {}\".format(x_column,y_column))\n",
    "\n",
    "    # Save plot\n",
    "    filename = os.path.join(output_path,\"Scatter plot of {} vs {}.png\".format(x_column,y_column))\n",
    "    fig.savefig(filename)\n",
    "\n",
    "def main():\n",
    "    print(\"Loading Data...\")\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data', type=str)\n",
    "    parser.add_argument('--plots_dir', type=str)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    diabetes = pd.read_csv(args.data, header= 0)\n",
    "\n",
    "    # plot correlations\n",
    "    plot_correlations(data=diabetes, output_path = args.plots_dir)\n",
    "\n",
    "    # plot distributions\n",
    "    exlude_column = set([\"Diabetic\", \"PatientID\"])\n",
    "    columns = diabetes.columns.values\n",
    "    for x in columns:\n",
    "        if x not in exlude_column:\n",
    "            plot_distribution(var_data=diabetes[x],column_name=x, output_path = args.plots_dir)\n",
    "\n",
    "    # plot scatter plots\n",
    "    columns = set(columns)\n",
    "    column_comb=list(combinations(columns-exlude_column,2))\n",
    "    column_comb = [list(x) for x in column_comb]\n",
    "\n",
    "    for x_y_pairs in column_comb:\n",
    "        plot_scatters(diabetes[x_y_pairs], output_path = args.plots_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training script\n",
    "\n",
    "To train a model, you'll first create the **diabetes_training.py** script in the **src** folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./components/train/diabetes-training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {components_dirs[\"train\"]}/diabetes-training.py\n",
    "# import libraries\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import logging\n",
    "import mlflow\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Parse job parameters\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--reg_rate', type=float, default=0.01)\n",
    "    parser.add_argument('--train_data', type=str, help=\"path to train data\")\n",
    "    parser.add_argument('--test_data', type=str, help=\"path to test data\")\n",
    "    parser.add_argument(\"--trained_model\", type=str,help=\"path to trained data\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Start logging\n",
    "    mlflow.start_run()\n",
    "    \n",
    "    # load the diabetes dataset\n",
    "    print(\"Loading Data...\")\n",
    "    train_file = os.path.join(args.train_data, \"train_data.csv\")\n",
    "    test_file = os.path.join(args.test_data, \"test_data.csv\")\n",
    "    train_df = pd.read_csv(train_file, header=0, index_col=\"PatientID\")\n",
    "    y_train = train_df.pop(\"Diabetic\")\n",
    "    x_train = train_df.values\n",
    "    test_df = pd.read_csv(test_file, header=0, index_col=\"PatientID\")\n",
    "    y_test = test_df.pop(\"Diabetic\")\n",
    "    x_test = test_df.values\n",
    "\n",
    "    print(\"num_train_samples:\", x_train.shape[0])\n",
    "    mlflow.log_metric(\"num_train_samples:\", x_train.shape[0])\n",
    "    print(\"num_test_samples:\", x_test.shape[0])\n",
    "    mlflow.log_metric(\"num_test_samples\", x_test.shape[0])\n",
    "    print(\"num_features:\", x_train.shape[1])\n",
    "    mlflow.log_metric(\"num_features\", x_train.shape[1])\n",
    "    print(\"features:\", train_df.columns.values)\n",
    "    mlflow.log_param(\"features\", train_df.columns.values)\n",
    "\n",
    "\n",
    "    # train a logistic regression model\n",
    "    print('Training a logistic regression model with regularization rate of', args.reg_rate)\n",
    "    mlflow.log_param(\"estimator\",\"LogisticRegression\")\n",
    "    mlflow.log_param(\"regularization_rate\", args.reg_rate)\n",
    "    mlflow.log_param(\"solver\", \"liblinear\")\n",
    "    model = LogisticRegression(C=1/args.reg_rate, solver=\"liblinear\").fit(x_train, y_train)\n",
    "\n",
    "    # calculate accuracy\n",
    "    y_hat = model.predict(x_test)\n",
    "    acc = np.average(y_hat == y_test)\n",
    "    print('Accuracy:', float(acc))\n",
    "    mlflow.log_metric('Accuracy:', float(acc))\n",
    "\n",
    "    # calculate AUC\n",
    "    y_scores = model.predict_proba(x_test)\n",
    "    auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "    print('AUC:', str(auc))\n",
    "    mlflow.log_metric('AUC:', str(auc))\n",
    "\n",
    "    # Save the model to file\n",
    "    print(\"Saving model to file\")\n",
    "    filename = os.path.join(args.trained_model,\"model.pkl\")\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(model,file)\n",
    "\n",
    "    # Stop logging\n",
    "    mlflow.end_run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dependencies_dir = \"./dependencies\"\n",
    "os.makedirs(dependencies_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./dependencies/conda.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {dependencies_dir}/conda.yaml\n",
    "name: sklearn-1.5\n",
    "channels:\n",
    "- conda-forge\n",
    "- anaconda\n",
    "dependencies:\n",
    "- python=3.10\n",
    "- pip=21.3.1\n",
    "- pandas~=1.5.3\n",
    "- scipy~=1.10.0\n",
    "- numpy~=1.22.0\n",
    "- pip:\n",
    "  - scikit-learn-intelex==2024.6.0\n",
    "  - azureml-core==1.57.0\n",
    "  - azureml-defaults==1.57.0\n",
    "  - azureml-mlflow==1.57.0.post1\n",
    "  - azureml-telemetry==1.57.0\n",
    "  - scikit-learn~=1.5.0\n",
    "  - joblib~=1.2.0\n",
    "  # azureml-automl-common-tools packages\n",
    "  - py-spy==0.3.12\n",
    "  - debugpy~=1.6.3\n",
    "  - ipykernel~=6.0\n",
    "  - tensorboard\n",
    "  - psutil~=5.8.0\n",
    "  - matplotlib~=3.5.0\n",
    "  - seaborn~=0.13.2\n",
    "  - tqdm~=4.66.3\n",
    "  - py-cpuinfo==5.0.0\n",
    "  - torch-tb-profiler~=0.4.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment with name diabetes-scikit-learn is registered to workspace, the environment version is 4\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "env_name = \"diabetes-scikit-learn\"\n",
    "\n",
    "env = Environment(\n",
    "    name=env_name,\n",
    "    description=\"Custom environment for Diabetes prediction pipeline\",\n",
    "    tags={\"scikit-learn\": \"1.5.0\"},\n",
    "    conda_file=os.path.join(dependencies_dir, \"conda.yaml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
    ")\n",
    "env = ml_client.environments.create_or_update(env)\n",
    "\n",
    "print(\n",
    "    f\"Environment with name {env.name} is registered to workspace, the environment version is {env.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component data_prep_diabetes with Version 2024-09-15-12-22-47-3858108 is registered\n",
      "Component eda_diabetes with Version 2024-09-15-12-22-51-8291496 is registered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading train (0.0 MBs): 100%|██████████| 2670/2670 [00:00<00:00, 16188.70it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component model_train_diabetes with Version 2024-09-15-12-22-57-9253183 is registered\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import command, Input, Output\n",
    "\n",
    "command_env = \"{0}:{1}\".format(env.name, env.version)\n",
    "experiment_name = \"diabetes-training\"\n",
    "\n",
    "data_prep_command = command(\n",
    "    name=\"data_prep_diabetes\",\n",
    "    display_name=\"Data preparation for training\",\n",
    "    description=\"Read the diabetes csv data and splits into train and test sets\",\n",
    "    inputs=dict(data=Input(type=\"uri_file\"), test_size=Input(type=\"number\")),\n",
    "    outputs=dict(\n",
    "        train_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        test_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    ),\n",
    "    code=components_dirs[\"data_prep\"],\n",
    "    command=\"python data_prep.py  --data ${{inputs.data}} --test_size ${{inputs.test_size}}  --train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}}\",\n",
    "    environment=command_env,\n",
    "    experiment_name=experiment_name,\n",
    ")\n",
    "\n",
    "# Now we register the component to the workspace\n",
    "data_prep_component = ml_client.create_or_update(data_prep_command.component)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {data_prep_component.name} with Version {data_prep_component.version} is registered\"\n",
    ")\n",
    "\n",
    "eda_command = command(\n",
    "    name=\"eda_diabetes\",\n",
    "    display_name=\"Exploratory data analysis\",\n",
    "    description=\"Reads the diabetes csv data and plot exploratory data analysis graphs\",\n",
    "    inputs=dict(data=Input(type=\"uri_file\")),\n",
    "    outputs=dict(plots_dir=Output(type=\"uri_folder\", mode=\"rw_mount\")),\n",
    "    code=components_dirs[\"eda\"],\n",
    "    command=\"python diabetes-exploratory-plots.py --data ${{inputs.data}} --plots ${{outputs.plots_dir}}\",\n",
    "    environment=command_env,\n",
    "    experiment_name=experiment_name,\n",
    ")\n",
    "\n",
    "# Now we register the component to the workspace\n",
    "eda_command_component = ml_client.create_or_update(eda_command.component)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {eda_command_component.name} with Version {eda_command_component.version} is registered\"\n",
    ")\n",
    "\n",
    "model_train_command = command(\n",
    "    name=\"model_train_diabetes\",\n",
    "    display_name=\"Train model\",\n",
    "    description=\"Train a Logistic Regression model diabetes dataset\",\n",
    "    inputs=dict(\n",
    "        train_data=Input(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        test_data=Input(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        reg_rate=Input(type=\"number\"),\n",
    "    ),\n",
    "    outputs=dict(trained_model=Output(type=\"uri_folder\", mode=\"rw_mount\")),\n",
    "    code=components_dirs[\"train\"],\n",
    "    command=\"python diabetes-training.py --train_data ${{inputs.train_data}} --test_data ${{inputs.test_data}} --reg_rate ${{inputs.reg_rate}} --trained_model ${{outputs.trained_model}}\",\n",
    "    environment=command_env,\n",
    "    experiment_name=experiment_name,\n",
    ")\n",
    "\n",
    "# Now we register the component to the workspace\n",
    "model_train_command_component = ml_client.create_or_update(\n",
    "    model_train_command.component\n",
    ")\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {model_train_command_component.name} with Version {model_train_command_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Pipeline\n",
    "Test data size (`test_size`) and Regularization rate (`reg_rate`) for the `LogisticRegression` are passed as parameters. Other parameters such as a registered dataset in a data store could also be passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import dsl, Input, Output\n",
    "\n",
    "# Define the pipeline\n",
    "@dsl.pipeline(compute=\"serverless\", description=\" Diabetes prediction pipeline\")\n",
    "def diabetes_pipeline(data_input, test_size, reg_rate):\n",
    "    data_prep_job =data_prep_component(\n",
    "                data = data_input,\n",
    "                test_size = test_size\n",
    "    )\n",
    "\n",
    "    eda_job = eda_command_component(\n",
    "        data = data_input\n",
    "    )\n",
    "\n",
    "    model_train_job = model_train_command_component(\n",
    "        train_data = data_prep_job.outputs.train_data,\n",
    "        test_data = data_prep_job.outputs.test_data,\n",
    "        reg_rate = reg_rate\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"train_data\": data_prep_job.outputs.train_data,\n",
    "        \"test_data\": data_prep_job.outputs.test_data,\n",
    "        \"eda_plots\": eda_job.outputs.plots_dir,\n",
    "        \"trained_model\": model_train_job.outputs.trained_model\n",
    "    }\n",
    "\n",
    "\n",
    "# Define arguments / parameters\n",
    "diabetes_data = ml_client.data.get(\"diabetes_csv\", version=\"1.0.0\")\n",
    "test_size = 0.30\n",
    "reg_rate = 0.01\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = diabetes_pipeline(\n",
    "    data_input=Input(type=\"uri_file\", path= diabetes_data.path),\n",
    "    test_size=test_size,\n",
    "    reg_rate=reg_rate\n",
    ")\n",
    "\n",
    "#submit the pipeline job\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    experiment_name= experiment_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "# Open job url\n",
    "webbrowser.open(pipeline_job.studio_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
